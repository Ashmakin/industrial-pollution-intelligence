{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Industrial Pollution Intelligence & Analysis\n",
        "\n",
        "## Comprehensive Scientific Analysis of Water Quality Data\n",
        "\n",
        "This notebook presents a rigorous scientific analysis of industrial pollution patterns using advanced machine learning techniques, causal inference, and product lifecycle assessment. The analysis leverages real-time water quality monitoring data from China's National Environmental Monitoring Center (CNEMC) to provide actionable insights for environmental policy and industrial management.\n",
        "\n",
        "### Key Innovations:\n",
        "- **Multi-dimensional Analysis**: PCA, t-SNE, and UMAP for pattern discovery\n",
        "- **Causal Inference**: Granger causality testing for pollution relationships\n",
        "- **Product Lifecycle Tracking**: Smartphone manufacturing pollution correlation\n",
        "- **Advanced Forecasting**: LSTM, Prophet, and ensemble models\n",
        "- **Network Analysis**: Pollution propagation and community detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scientific computing\n",
        "from scipy import stats\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Time series analysis\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Machine learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Network analysis\n",
        "import networkx as nx\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Preprocessing\n",
        "\n",
        "### 1.1 Load Processed Water Quality Data\n",
        "\n",
        "The dataset contains 4-hour interval water quality measurements from monitoring stations across China, including:\n",
        "- **Physical parameters**: Temperature, pH, dissolved oxygen, conductivity, turbidity\n",
        "- **Chemical parameters**: Ammonia nitrogen, total phosphorus, total nitrogen, permanganate index\n",
        "- **Biological parameters**: Chlorophyll-a, algae density\n",
        "- **Spatial metadata**: Province, watershed, monitoring station locations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed water quality data\n",
        "df = pd.read_parquet('data/processed_water_quality.parquet')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Date range: {df['monitoring_time'].min()} to {df['monitoring_time'].max()}\")\n",
        "print(f\"Number of stations: {df['station_name'].nunique()}\")\n",
        "print(f\"Number of provinces: {df['province'].nunique()}\")\n",
        "print(f\"Number of watersheds: {df['watershed'].nunique()}\")\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\nDataset Overview:\")\n",
        "print(df.info())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percentage = (missing_data / len(df)) * 100\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Missing Count': missing_data,\n",
        "    'Missing Percentage': missing_percentage\n",
        "})\n",
        "print(missing_summary[missing_summary['Missing Count'] > 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Data Quality Assessment\n",
        "\n",
        "Assess data quality through statistical analysis and visualization of key water quality parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define key water quality parameters\n",
        "water_quality_params = [\n",
        "    'temperature', 'ph', 'dissolved_oxygen', 'conductivity', 'turbidity',\n",
        "    'permanganate_index', 'ammonia_nitrogen', 'total_phosphorus', 'total_nitrogen',\n",
        "    'chlorophyll_a', 'algae_density'\n",
        "]\n",
        "\n",
        "# Filter available parameters\n",
        "available_params = [param for param in water_quality_params if param in df.columns]\n",
        "print(f\"Available water quality parameters: {available_params}\")\n",
        "\n",
        "# Calculate descriptive statistics\n",
        "desc_stats = df[available_params].describe()\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(desc_stats.round(3))\n",
        "\n",
        "# Create distribution plots\n",
        "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, param in enumerate(available_params):\n",
        "    if i < len(axes):\n",
        "        # Remove outliers for better visualization (using IQR method)\n",
        "        Q1 = df[param].quantile(0.25)\n",
        "        Q3 = df[param].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        \n",
        "        filtered_data = df[(df[param] >= lower_bound) & (df[param] <= upper_bound)][param]\n",
        "        \n",
        "        axes[i].hist(filtered_data.dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
        "        axes[i].set_title(f'{param.replace(\"_\", \" \").title()}')\n",
        "        axes[i].set_xlabel('Value')\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "# Hide empty subplots\n",
        "for i in range(len(available_params), len(axes)):\n",
        "    axes[i].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Box plots for outlier detection\n",
        "fig, ax = plt.subplots(figsize=(15, 8))\n",
        "df[available_params].boxplot(ax=ax, rot=45)\n",
        "ax.set_title('Water Quality Parameters - Box Plot Analysis')\n",
        "ax.set_ylabel('Parameter Value')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Temporal Analysis and Trend Detection\n",
        "\n",
        "### 2.1 Seasonal Patterns in Water Quality\n",
        "\n",
        "Analyze seasonal variations in water quality parameters to identify natural cycles and potential anthropogenic influences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract temporal features\n",
        "df['year'] = df['monitoring_time'].dt.year\n",
        "df['month'] = df['monitoring_time'].dt.month\n",
        "df['day_of_year'] = df['monitoring_time'].dt.dayofyear\n",
        "df['hour'] = df['monitoring_time'].dt.hour\n",
        "df['season'] = df['month'].map({12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
        "                               3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
        "                               6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
        "                               9: 'Autumn', 10: 'Autumn', 11: 'Autumn'})\n",
        "\n",
        "# Seasonal analysis for key parameters\n",
        "key_params = ['temperature', 'dissolved_oxygen', 'ammonia_nitrogen', 'total_phosphorus', 'chlorophyll_a']\n",
        "seasonal_data = df.groupby(['season', 'month'])[key_params].mean().reset_index()\n",
        "\n",
        "# Create seasonal plots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, param in enumerate(key_params):\n",
        "    if i < len(axes):\n",
        "        # Seasonal box plot\n",
        "        df.boxplot(column=param, by='season', ax=axes[i])\n",
        "        axes[i].set_title(f'{param.replace(\"_\", \" \").title()} by Season')\n",
        "        axes[i].set_xlabel('Season')\n",
        "        axes[i].set_ylabel('Value')\n",
        "\n",
        "# Hide empty subplot\n",
        "axes[5].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Monthly trends\n",
        "monthly_avg = df.groupby('month')[key_params].mean()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 8))\n",
        "for param in key_params:\n",
        "    ax.plot(monthly_avg.index, monthly_avg[param], marker='o', label=param.replace('_', ' ').title(), linewidth=2)\n",
        "\n",
        "ax.set_title('Monthly Average Water Quality Parameters')\n",
        "ax.set_xlabel('Month')\n",
        "ax.set_ylabel('Average Value')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Principal Component Analysis (PCA)\n",
        "\n",
        "### 3.1 Dimensionality Reduction and Pattern Discovery\n",
        "\n",
        "Apply PCA to identify the main drivers of water quality variation and reduce dimensionality while preserving maximum variance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for PCA\n",
        "pca_data = df[available_params].dropna()\n",
        "print(f\"Data for PCA: {pca_data.shape}\")\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "pca_data_scaled = scaler.fit_transform(pca_data)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=min(10, len(available_params)))\n",
        "pca_result = pca.fit_transform(pca_data_scaled)\n",
        "\n",
        "# Explained variance analysis\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "print(f\"\\nExplained Variance Ratio:\")\n",
        "for i, (var_ratio, cum_var) in enumerate(zip(explained_variance_ratio, cumulative_variance)):\n",
        "    print(f\"PC{i+1}: {var_ratio:.3f} ({var_ratio*100:.1f}%) | Cumulative: {cum_var:.3f} ({cum_var*100:.1f}%)\")\n",
        "\n",
        "# Plot explained variance\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Individual explained variance\n",
        "ax1.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, \n",
        "        alpha=0.7, color='skyblue', edgecolor='navy')\n",
        "ax1.set_title('Explained Variance by Principal Component')\n",
        "ax1.set_xlabel('Principal Component')\n",
        "ax1.set_ylabel('Explained Variance Ratio')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Cumulative explained variance\n",
        "ax2.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, \n",
        "         marker='o', linewidth=2, markersize=8, color='red')\n",
        "ax2.axhline(y=0.95, color='green', linestyle='--', alpha=0.7, label='95% Variance')\n",
        "ax2.axhline(y=0.80, color='orange', linestyle='--', alpha=0.7, label='80% Variance')\n",
        "ax2.set_title('Cumulative Explained Variance')\n",
        "ax2.set_xlabel('Number of Components')\n",
        "ax2.set_ylabel('Cumulative Explained Variance Ratio')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Granger Causality Analysis\n",
        "\n",
        "### 4.1 Causal Relationships in Water Quality Parameters\n",
        "\n",
        "Apply Granger causality tests to identify causal relationships between different water quality parameters, helping understand pollution propagation mechanisms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "def perform_granger_test(data, cause_var, effect_var, max_lags=6):\n",
        "    \"\"\"Perform Granger causality test between two variables\"\"\"\n",
        "    try:\n",
        "        # Prepare data\n",
        "        test_data = data[[effect_var, cause_var]].dropna()\n",
        "        \n",
        "        if len(test_data) < 30:  # Need sufficient data\n",
        "            return None\n",
        "        \n",
        "        # Perform Granger test\n",
        "        results = grangercausalitytests(test_data, maxlag=max_lags, verbose=False)\n",
        "        \n",
        "        # Extract p-values for each lag\n",
        "        p_values = []\n",
        "        for lag in range(1, max_lags + 1):\n",
        "            if lag in results:\n",
        "                p_val = results[lag][0]['ssr_ftest'][1]\n",
        "                p_values.append(p_val)\n",
        "        \n",
        "        # Find best lag (minimum p-value)\n",
        "        if p_values:\n",
        "            best_lag = np.argmin(p_values) + 1\n",
        "            best_p_value = min(p_values)\n",
        "            \n",
        "            return {\n",
        "                'cause_var': cause_var,\n",
        "                causes_var': effect_var,\n",
        "                'best_lag': best_lag,\n",
        "                'p_value': best_p_value,\n",
        "                'significant': best_p_value < 0.05,\n",
        "                'all_p_values': p_values\n",
        "            }\n",
        "        \n",
        "        return None\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error in Granger test for {cause_var} -> {effect_var}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define variable pairs for causality testing\n",
        "causality_pairs = [\n",
        "    ('ammonia_nitrogen', 'total_phosphorus'),\n",
        "    ('total_phosphorus', 'chlorophyll_a'),\n",
        "    ('total_nitrogen', 'chlorophyll_a'),\n",
        "    ('temperature', 'dissolved_oxygen'),\n",
        "    ('ph', 'ammonia_nitrogen'),\n",
        "    ('turbidity', 'algae_density'),\n",
        "    ('conductivity', 'total_phosphorus')\n",
        "]\n",
        "\n",
        "# Perform Granger causality tests\n",
        "granger_results = []\n",
        "\n",
        "for station in df['station_name'].unique()[:5]:  # Limit to 5 stations for demo\n",
        "    station_data = df[df['station_name'] == station].sort_values('monitoring_time')\n",
        "    \n",
        "    for cause_var, effect_var in causality_pairs:\n",
        "        if cause_var in station_data.columns and effect_var in station_data.columns:\n",
        "            result = perform_granger_test(station_data, cause_var, effect_var)\n",
        "            if result:\n",
        "                result['station'] = station\n",
        "                granger_results.append(result)\n",
        "\n",
        "granger_df = pd.DataFrame(granger_results)\n",
        "\n",
        "# Summary of Granger causality results\n",
        "if not granger_df.empty:\n",
        "    print(\"Granger Causality Analysis Results:\")\n",
        "    print(f\"Total tests performed: {len(granger_df)}\")\n",
        "    print(f\"Significant relationships: {granger_df['significant'].sum()}\")\n",
        "    \n",
        "    # Display significant results\n",
        "    significant_results = granger_df[granger_df['significant']]\n",
        "    if not significant_results.empty:\n",
        "        print(\"\\nSignificant Causal Relationships:\")\n",
        "        for _, row in significant_results.iterrows():\n",
        "            print(f\"{row['cause_var']} -> {row['effect_var']} (Station: {row['station']})\")\n",
        "            print(f\"  Lag: {row['best_lag']}, P-value: {row['p_value']:.4f}\")\n",
        "    \n",
        "    # Visualize p-values\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    # Create a heatmap of p-values\n",
        "    causality_matrix = granger_df.groupby(['cause_var', 'effect_var'])['p_value'].mean().unstack()\n",
        "    \n",
        "    if not causality_matrix.empty:\n",
        "        sns.heatmap(causality_matrix, annot=True, cmap='RdYlBu_r', \n",
        "                   center=0.05, vmin=0, vmax=1, ax=ax)\n",
        "        ax.set_title('Granger Causality P-Values Heatmap')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import lifecycle tracking modules\n",
        "import sys\n",
        "sys.path.append('../python')\n",
        "from lifecycle.product_trace import SmartphoneLifecycleTracker, WaterQualityCorrelationAnalyzer\n",
        "\n",
        "# Initialize lifecycle tracker\n",
        "tracker = SmartphoneLifecycleTracker()\n",
        "\n",
        "# Calculate total pollution per phone\n",
        "total_pollution = tracker.calculate_total_pollution_per_phone()\n",
        "print(\"Total pollution per smartphone:\")\n",
        "for pollutant, amount in total_pollution.items():\n",
        "    print(f\"  {pollutant}: {amount:.3f}\")\n",
        "\n",
        "# Estimate regional pollution impact (1 million phones)\n",
        "regional_impact = tracker.estimate_regional_pollution_impact(production_volume=1000000)\n",
        "print(f\"\\nRegional pollution impact (1M phones):\")\n",
        "for location, impact in regional_impact.items():\n",
        "    print(f\"\\n{location}:\")\n",
        "    for pollutant, amount in impact['pollutants'].items():\n",
        "        if amount > 0:\n",
        "            print(f\"  {pollutant}: {amount:,.1f} kg\")\n",
        "\n",
        "# Analyze correlations with water quality\n",
        "analyzer = WaterQualityCorrelationAnalyzer(tracker)\n",
        "correlations = analyzer.analyze_temporal_correlations(df)\n",
        "\n",
        "print(f\"\\nFound {len(correlations)} correlations between manufacturing and water quality\")\n",
        "significant_correlations = correlations[correlations['significant']]\n",
        "print(f\"Significant correlations: {len(significant_correlations)}\")\n",
        "\n",
        "# Identify pollution hotspots\n",
        "hotspots = analyzer.identify_pollution_hotspots(df)\n",
        "print(f\"\\nPollution hotspots identified: {len(hotspots)}\")\n",
        "for hotspot in hotspots[:3]:  # Top 3\n",
        "    print(f\"  {hotspot['zone']}: avg_correlation={hotspot['avg_correlation']:.3f}\")\n",
        "\n",
        "# Generate network data for visualization\n",
        "network_data = tracker.create_supply_chain_network_data()\n",
        "print(f\"\\nSupply chain network: {network_data['metadata']['total_nodes']} nodes, {network_data['metadata']['total_links']} links\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Advanced Machine Learning Models\n",
        "\n",
        "### 6.1 LSTM Time Series Forecasting\n",
        "\n",
        "Implement deep learning models for multi-step ahead forecasting of water quality parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import forecasting modules\n",
        "from ml.forecasting.lstm_model import WaterQualityLSTM, MultiTargetLSTM\n",
        "from ml.forecasting.prophet_model import WaterQualityProphet\n",
        "\n",
        "# Prepare data for forecasting\n",
        "forecast_data = df[df['station_name'].isin(df['station_name'].unique()[:3])].copy()  # Use 3 stations for demo\n",
        "forecast_data = forecast_data.sort_values(['station_name', 'monitoring_time'])\n",
        "\n",
        "# Define target variables for forecasting\n",
        "target_variables = ['ph', 'dissolved_oxygen', 'ammonia_nitrogen', 'total_phosphorus']\n",
        "\n",
        "# Train LSTM model\n",
        "print(\"Training LSTM model...\")\n",
        "lstm_model = WaterQualityLSTM(sequence_length=24, prediction_horizon=6)\n",
        "lstm_model.train(forecast_data, target_variables, validation_split=0.2, epochs=50)\n",
        "\n",
        "# Make predictions\n",
        "predictions = lstm_model.predict(forecast_data)\n",
        "print(f\"Generated predictions for {len(predictions)} stations\")\n",
        "\n",
        "# Train Prophet model\n",
        "print(\"\\nTraining Prophet model...\")\n",
        "prophet_model = WaterQualityProphet(\n",
        "    yearly_seasonality=True,\n",
        "    weekly_seasonality=True,\n",
        "    daily_seasonality=False,\n",
        "    changepoint_prior_scale=0.05\n",
        ")\n",
        "\n",
        "prophet_model.train_multi_station(forecast_data, target_variables)\n",
        "prophet_forecasts = prophet_model.forecast(forecast_data, target_variables, periods=168, freq='4H')\n",
        "\n",
        "print(f\"Prophet forecasts generated for {len(prophet_forecasts)} variables\")\n",
        "\n",
        "# Evaluate models\n",
        "print(\"\\nEvaluating model performance...\")\n",
        "lstm_metrics = lstm_model.evaluate(forecast_data, test_split=0.2)\n",
        "print(\"LSTM Model Metrics:\")\n",
        "for metric, value in lstm_metrics.items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "prophet_metrics = prophet_model.evaluate_all_models(forecast_data, target_variables, test_days=30)\n",
        "print(f\"\\nProphet Model Metrics:\")\n",
        "for target, station_metrics in prophet_metrics.items():\n",
        "    if station_metrics:\n",
        "        avg_rmse = np.mean([m['rmse'] for m in station_metrics.values()])\n",
        "        print(f\"  {target}: Average RMSE = {avg_rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Network Analysis and Community Detection\n",
        "\n",
        "### 7.1 Pollution Propagation Network\n",
        "\n",
        "Build network models to understand pollution propagation patterns and identify critical monitoring stations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pollution correlation network\n",
        "def build_pollution_network(df, threshold=0.5):\n",
        "    \"\"\"Build network based on pollution correlations\"\"\"\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    # Calculate correlations between stations for each parameter\n",
        "    stations = df['station_name'].unique()[:10]  # Limit for demo\n",
        "    \n",
        "    for param in key_params:\n",
        "        if param in df.columns:\n",
        "            param_data = df[df['station_name'].isin(stations)].pivot_table(\n",
        "                index='monitoring_time', \n",
        "                columns='station_name', \n",
        "                values=param\n",
        "            ).corr()\n",
        "            \n",
        "            # Add edges for high correlations\n",
        "            for i, station1 in enumerate(param_data.index):\n",
        "                for j, station2 in enumerate(param_data.columns):\n",
        "                    if i < j and not pd.isna(param_data.loc[station1, station2]):\n",
        "                        corr = abs(param_data.loc[station1, station2])\n",
        "                        if corr > threshold:\n",
        "                            if G.has_edge(station1, station2):\n",
        "                                G[station1][station2]['weight'] += corr\n",
        "                                G[station1][station2]['parameters'].append(param)\n",
        "                            else:\n",
        "                                G.add_edge(station1, station2, weight=corr, parameters=[param])\n",
        "    \n",
        "    return G\n",
        "\n",
        "# Build network\n",
        "pollution_network = build_pollution_network(df, threshold=0.3)\n",
        "\n",
        "print(f\"Network Statistics:\")\n",
        "print(f\"  Nodes (stations): {pollution_network.number_of_nodes()}\")\n",
        "print(f\"  Edges (correlations): {pollution_network.number_of_edges()}\")\n",
        "\n",
        "# Calculate network metrics\n",
        "if pollution_network.number_of_nodes() > 0:\n",
        "    # Degree centrality\n",
        "    degree_centrality = nx.degree_centrality(pollution_network)\n",
        "    betweenness_centrality = nx.betweenness_centrality(pollution_network)\n",
        "    closeness_centrality = nx.closeness_centrality(pollution_network)\n",
        "    \n",
        "    # Community detection using Louvain algorithm\n",
        "    try:\n",
        "        import networkx.algorithms.community as nx_comm\n",
        "        communities = nx_comm.louvain_communities(pollution_network)\n",
        "        print(f\"  Communities detected: {len(communities)}\")\n",
        "        \n",
        "        # Identify most central stations\n",
        "        top_stations = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "        print(f\"\\nMost Central Stations (by degree):\")\n",
        "        for station, centrality in top_stations:\n",
        "            print(f\"  {station}: {centrality:.3f}\")\n",
        "            \n",
        "    except ImportError:\n",
        "        print(\"NetworkX community module not available\")\n",
        "    \n",
        "    # Visualize network\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    pos = nx.spring_layout(pollution_network, k=1, iterations=50)\n",
        "    \n",
        "    # Draw nodes with size based on degree centrality\n",
        "    node_sizes = [degree_centrality[node] * 1000 for node in pollution_network.nodes()]\n",
        "    nx.draw_networkx_nodes(pollution_network, pos, node_size=node_sizes, \n",
        "                          node_color='lightblue', alpha=0.7)\n",
        "    \n",
        "    # Draw edges with width based on correlation strength\n",
        "    edges = pollution_network.edges()\n",
        "    edge_weights = [pollution_network[u][v]['weight'] for u, v in edges]\n",
        "    nx.draw_networkx_edges(pollution_network, pos, width=[w*3 for w in edge_weights], \n",
        "                          alpha=0.5, edge_color='gray')\n",
        "    \n",
        "    # Draw labels\n",
        "    nx.draw_networkx_labels(pollution_network, pos, font_size=8)\n",
        "    \n",
        "    plt.title('Pollution Correlation Network\\n(Node size = Degree centrality, Edge width = Correlation strength)')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Anomaly Detection and Outlier Analysis\n",
        "\n",
        "### 8.1 Advanced Anomaly Detection Methods\n",
        "\n",
        "Apply multiple anomaly detection techniques to identify unusual pollution events and potential environmental incidents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Anomaly detection using multiple methods\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "def detect_anomalies_comprehensive(data, contamination=0.1):\n",
        "    \"\"\"Apply multiple anomaly detection methods\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    # Prepare data (remove non-numeric columns and handle missing values)\n",
        "    numeric_data = data.select_dtypes(include=[np.number]).fillna(method='ffill').fillna(method='bfill')\n",
        "    \n",
        "    if len(numeric_data) < 100:  # Need sufficient data\n",
        "        return results\n",
        "    \n",
        "    # Method 1: Isolation Forest\n",
        "    iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
        "    iso_anomalies = iso_forest.fit_predict(numeric_data)\n",
        "    results['isolation_forest'] = iso_anomalies == -1\n",
        "    \n",
        "    # Method 2: One-Class SVM\n",
        "    try:\n",
        "        oc_svm = OneClassSVM(gamma='scale', nu=contamination)\n",
        "        svm_anomalies = oc_svm.fit_predict(numeric_data)\n",
        "        results['one_class_svm'] = svm_anomalies == -1\n",
        "    except:\n",
        "        results['one_class_svm'] = np.zeros(len(numeric_data), dtype=bool)\n",
        "    \n",
        "    # Method 3: Statistical outliers (Z-score)\n",
        "    z_scores = np.abs(stats.zscore(numeric_data, axis=0, nan_policy='omit'))\n",
        "    z_threshold = 3\n",
        "    results['statistical_outliers'] = (z_scores > z_threshold).any(axis=1)\n",
        "    \n",
        "    # Method 4: DBSCAN clustering\n",
        "    try:\n",
        "        # Use PCA to reduce dimensionality for DBSCAN\n",
        "        pca = PCA(n_components=min(5, numeric_data.shape[1]))\n",
        "        data_pca = pca.fit_transform(numeric_data.fillna(numeric_data.mean()))\n",
        "        \n",
        "        dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "        cluster_labels = dbscan.fit_predict(data_pca)\n",
        "        results['dbscan_outliers'] = cluster_labels == -1\n",
        "    except:\n",
        "        results['dbscan_outliers'] = np.zeros(len(numeric_data), dtype=bool)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Apply anomaly detection to each station\n",
        "anomaly_results = []\n",
        "for station in df['station_name'].unique()[:5]:  # Limit for demo\n",
        "    station_data = df[df['station_name'] == station].sort_values('monitoring_time')\n",
        "    \n",
        "    if len(station_data) > 100:\n",
        "        anomalies = detect_anomalies_comprehensive(station_data)\n",
        "        \n",
        "        for method, anomaly_mask in anomalies.items():\n",
        "            anomaly_count = anomaly_mask.sum()\n",
        "            if anomaly_count > 0:\n",
        "                anomaly_results.append({\n",
        "                    'station': station,\n",
        "                    'method': method,\n",
        "                    'anomaly_count': anomaly_count,\n",
        "                    'anomaly_rate': anomaly_count / len(anomaly_mask)\n",
        "                })\n",
        "\n",
        "anomaly_df = pd.DataFrame(anomaly_results)\n",
        "\n",
        "if not anomaly_df.empty:\n",
        "    print(\"Anomaly Detection Results:\")\n",
        "    print(anomaly_df.groupby('method')['anomaly_count'].agg(['sum', 'mean']))\n",
        "    \n",
        "    # Visualize anomaly detection results\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    methods = anomaly_df['method'].unique()\n",
        "    for i, method in enumerate(methods[:4]):\n",
        "        if i < len(axes):\n",
        "            method_data = anomaly_df[anomaly_df['method'] == method]\n",
        "            axes[i].bar(method_data['station'], method_data['anomaly_count'])\n",
        "            axes[i].set_title(f'Anomalies Detected - {method.replace(\"_\", \" \").title()}')\n",
        "            axes[i].set_xlabel('Station')\n",
        "            axes[i].set_ylabel('Number of Anomalies')\n",
        "            axes[i].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Hide empty subplots\n",
        "    for i in range(len(methods), len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Conclusions and Policy Recommendations\n",
        "\n",
        "### 9.1 Key Findings Summary\n",
        "\n",
        "Based on our comprehensive analysis of water quality data, machine learning models, and product lifecycle assessment, we present the following key findings and recommendations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive summary of findings\n",
        "print(\"=\"*80)\n",
        "print(\"INDUSTRIAL POLLUTION INTELLIGENCE - KEY FINDINGS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Data Overview\n",
        "print(f\"\\n1. DATA OVERVIEW:\")\n",
        "print(f\"   • Total monitoring stations: {df['station_name'].nunique()}\")\n",
        "print(f\"   • Total measurements: {len(df):,}\")\n",
        "print(f\"   • Date range: {df['monitoring_time'].min().strftime('%Y-%m-%d')} to {df['monitoring_time'].max().strftime('%Y-%m-%d')}\")\n",
        "print(f\"   • Geographic coverage: {df['province'].nunique()} provinces, {df['watershed'].nunique()} watersheds\")\n",
        "\n",
        "# 2. Water Quality Status\n",
        "if 'water_quality_grade' in df.columns:\n",
        "    grade_dist = df['water_quality_grade'].value_counts()\n",
        "    print(f\"\\n2. WATER QUALITY STATUS:\")\n",
        "    for grade, count in grade_dist.head().items():\n",
        "        percentage = (count / len(df)) * 100\n",
        "        print(f\"   • Grade {grade}: {count:,} measurements ({percentage:.1f}%)\")\n",
        "\n",
        "# 3. Critical Pollutants\n",
        "print(f\"\\n3. CRITICAL POLLUTANTS (by variability):\")\n",
        "param_variability = df[available_params].std().sort_values(ascending=False)\n",
        "for param, std_val in param_variability.head(5).items():\n",
        "    print(f\"   • {param.replace('_', ' ').title()}: σ = {std_val:.3f}\")\n",
        "\n",
        "# 4. Seasonal Patterns\n",
        "print(f\"\\n4. SEASONAL PATTERNS:\")\n",
        "if 'season' in df.columns:\n",
        "    seasonal_summary = df.groupby('season')[key_params].mean()\n",
        "    for param in key_params[:3]:  # Top 3 parameters\n",
        "        if param in seasonal_summary.columns:\n",
        "            winter_val = seasonal_summary.loc['Winter', param]\n",
        "            summer_val = seasonal_summary.loc['Summer', param]\n",
        "            change = ((summer_val - winter_val) / winter_val) * 100\n",
        "            print(f\"   • {param.replace('_', ' ').title()}: {change:+.1f}% change (Winter → Summer)\")\n",
        "\n",
        "# 5. Forecasting Performance\n",
        "if 'lstm_metrics' in locals():\n",
        "    print(f\"\\n5. FORECASTING PERFORMANCE:\")\n",
        "    for metric, value in lstm_metrics.items():\n",
        "        print(f\"   • {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
        "\n",
        "# 6. Product Lifecycle Impact\n",
        "print(f\"\\n6. PRODUCT LIFECYCLE IMPACT:\")\n",
        "if 'total_pollution' in locals():\n",
        "    print(f\"   • Total water usage per smartphone: {total_pollution['water_usage']:,} liters\")\n",
        "    print(f\"   • Total energy consumption: {total_pollution['energy_consumption']:.1f} kWh\")\n",
        "    print(f\"   • Total waste generation: {total_pollution['waste_generation']:.3f} kg\")\n",
        "    \n",
        "    # Top pollutants\n",
        "    pollutant_impacts = {k: v for k, v in total_pollution.items() \n",
        "                        if k not in ['water_usage', 'energy_consumption', 'waste_generation']}\n",
        "    top_pollutant = max(pollutant_impacts.items(), key=lambda x: x[1])\n",
        "    print(f\"   • Highest pollutant: {top_pollutant[0]} ({top_pollutant[1]:.4f} kg per phone)\")\n",
        "\n",
        "# 7. Network Analysis\n",
        "if 'pollution_network' in locals() and pollution_network.number_of_nodes() > 0:\n",
        "    print(f\"\\n7. NETWORK ANALYSIS:\")\n",
        "    print(f\"   • Stations in network: {pollution_network.number_of_nodes()}\")\n",
        "    print(f\"   • Correlation connections: {pollution_network.number_of_edges()}\")\n",
        "    avg_degree = sum(dict(pollution_network.degree()).values()) / pollution_network.number_of_nodes()\n",
        "    print(f\"   • Average connections per station: {avg_degree:.1f}\")\n",
        "\n",
        "# 8. Anomaly Detection\n",
        "if not anomaly_df.empty:\n",
        "    print(f\"\\n8. ANOMALY DETECTION:\")\n",
        "    total_anomalies = anomaly_df['anomaly_count'].sum()\n",
        "    print(f\"   • Total anomalies detected: {total_anomalies:,}\")\n",
        "    best_method = anomaly_df.groupby('method')['anomaly_count'].sum().idxmax()\n",
        "    print(f\"   • Most effective method: {best_method.replace('_', ' ').title()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RECOMMENDATIONS:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "recommendations = [\n",
        "    \"1. MONITORING NETWORK OPTIMIZATION:\",\n",
        "    \"   • Deploy additional sensors in high-correlation zones identified by network analysis\",\n",
        "    \"   • Implement real-time anomaly detection using LSTM models\",\n",
        "    \"   • Focus monitoring on critical pollutants with high variability\",\n",
        "    \"\",\n",
        "    \"2. INDUSTRIAL REGULATION:\",\n",
        "    \"   • Target smartphone manufacturing zones for stricter pollution controls\",\n",
        "    \"   • Implement closed-loop water systems in electronics manufacturing\",\n",
        "    \"   • Mandate pollution tracking across supply chains\",\n",
        "    \"\",\n",
        "    \"3. PREDICTIVE MANAGEMENT:\",\n",
        "    \"   • Use ensemble forecasting models for early warning systems\",\n",
        "    \"   • Implement seasonal adjustment protocols based on temporal patterns\",\n",
        "    \"   • Develop pollution propagation models for upstream-downstream management\",\n",
        "    \"\",\n",
        "    \"4. POLICY INTERVENTIONS:\",\n",
        "    \"   • Focus on eutrophication control (ammonia nitrogen, total phosphorus)\",\n",
        "    \"   • Implement targeted interventions in identified pollution hotspots\",\n",
        "    \"   • Develop circular economy frameworks for product lifecycle management\"\n",
        "]\n",
        "\n",
        "for rec in recommendations:\n",
        "    print(rec)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
